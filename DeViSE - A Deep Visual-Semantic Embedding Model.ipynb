{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeViSE - A Deep Visual-Semantic Embedding Model\n",
    "\n",
    "Paper: [Frome et al. 2013](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "import fastText as ft\n",
    "import torchvision.transforms as transforms\n",
    "from fastai.io import get_data\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "### Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/CLS-LOC')\n",
    "TMP_PATH = PATH/'tmp'\n",
    "TRANS_PATH = Path('data/translate/word_vectors')\n",
    "PATH_TRAIN = PATH/'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttfms, vtfms = tfms_from_model(arch, sz, transforms_side_on, max_zoom=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.get_word_vector(\"cat\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.    , 0.0998],\n",
       "       [0.0998, 1.    ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(wordvectors.get_word_vector(\"house\"), wordvectors.get_word_vector(\"bug\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 0.64877],\n",
       "       [0.64877, 1.     ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(wordvectors.get_word_vector(\"queen\"), wordvectors.get_word_vector(\"king\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping imagenet classes to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wordvectors.get_words(include_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequencies_dict = {k:v for k,v in zip(*words)}  # contains word and respective frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(words_frequencies_dict.keys(), key=lambda x: words_frequencies_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)  # contains all fastText english words in ascending order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of the names of the 1000 imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_FN = 'imagenet_class_index.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(f'http://files.fast.ai/models/{CLASSES_FN}', TMP_PATH/CLASSES_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classIds_imagenet = dict(json.load((TMP_PATH/CLASSES_FN).open()).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goldfish'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classIds_imagenet['n01443537']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes in imagenet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classIds_imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of 'all' english nouns according to WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_FN = 'classids.txt'\n",
    "get_data(f'http://files.fast.ai/data/{WORDS_FN}', TMP_PATH/WORDS_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classid_nouns = (TMP_PATH/WORDS_FN).open().readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n00001740 entity\\n',\n",
       " 'n00001930 physical_entity\\n',\n",
       " 'n00002137 abstraction\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid_nouns[:3]  # Look at the format. Imagenet uses WordNet classes :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classIds_wordnet = dict(l.strip().split() for l in classid_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes in wordnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82115"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classIds_wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the ~80k wordnet ids to their respective wordvectors\n",
    "First, create a dictionary that maps the million most frequent words (in fastText) to their respective wordvector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_dict = {w.lower() : wordvectors.get_word_vector(w) for w in words[-1000000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_dict['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_dict[\"fish\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetId_2_wordvec = [(key, wordvec_dict[val.lower()]) for key, val in classIds_wordnet.items() if val.lower() in wordvec_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49469"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnetId_2_wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetId_2_wordvec_dict = dict(wordnetId_2_wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We found the wordvectors for ~49k words in WordNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the 1000 imagenet ids to their respective wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetId_2_wordvec = [(key, wordvec_dict[val.lower()]) for key, val in classIds_imagenet.items() if val.lower() in wordvec_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagenetId_2_wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We found the wordvectors for 574 classes in imagenet.**\n",
    "\n",
    "**Let's finally create a mapping of the ~49k words in wordnet for which we found a wordvector in fastText to their respective wordvector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "className_2_wordvec = [[classIds_wordnet[id], vec] for id, vec in wordnetId_2_wordvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates. This has bad time complexity obviously but it works in ~5s so I'll ignore this for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "className_2_wordvec_without_dups = []\n",
    "encountered_classNames = []\n",
    "\n",
    "for word, wordvec in className_2_wordvec:\n",
    "    if encountered_classNames.count(word) == 0:\n",
    "        className_2_wordvec_without_dups.append([word, wordvec])\n",
    "        encountered_classNames.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(className_2_wordvec_without_dups, (PATH/'className_2_wordvec_without_dups.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for training\n",
    "The inputs are images from imagenet, the labels are wordvectors from FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = []\n",
    "label_wordvecs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in PATH_TRAIN.iterdir():\n",
    "    if d.name not in wordnetId_2_wordvec_dict: continue\n",
    "    wordvec = wordnetId_2_wordvec_dict[d.name]\n",
    "    for f in d.iterdir():\n",
    "        input_images.append(str(f.relative_to(PATH)))\n",
    "        label_wordvecs.append(wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train/n01667778/n01667778_17582.JPEG',\n",
       " 'train/n01667778/n01667778_4936.JPEG',\n",
       " 'train/n01667778/n01667778_3675.JPEG']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739526, 739526)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_images), len(label_wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_wordvecs = np.stack(label_wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739526, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_wordvecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indcs = get_cv_idxs(len(input_images), val_pct=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_model(arch, sz, transforms_side_on, max_zoom=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldata = ImageClassifierData.from_names_and_array(PATH, input_images, label_wordvecs, val_idxs=val_indcs, classes=None, tfms=tfms, continuous=True, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(modeldata.val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(nn.BatchNorm1d(512),\n",
    "                         nn.Dropout(p),\n",
    "                         nn.Linear(in_features=512, out_features=512, bias=True),\n",
    "                         nn.ReLU(),\n",
    "                         nn.BatchNorm1d(512),\n",
    "                         nn.Dropout(p),\n",
    "                         nn.Linear(in_features=512, out_features=300, bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(modeldata, SingleModel(to_gpu(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(optim.Adam, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(input, target):\n",
    "    return 1 - F.cosine_similarity(input, target).mean()\n",
    "\n",
    "# does not care about th lengths of the vectors, only the angle between them\n",
    "# cosine sim returns 1 if two vectors point in same direction, 0 if angle is 90 degrees\n",
    "# to convert to loss we do 1 - cosine sim so that the loss is smaller when the angle is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.crit = cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd500284bf84d159e539e1a28cc7f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                  \n",
      "    0      1.0        1.0       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-4, end_lr=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEOCAYAAABmVAtTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XPV57/HPo5Fk2fJuy+AN24BZHHbEFkJwKOEaQk0ClKWhNyEkLm1MFtLcwG0uTQn3Nm2apCGQtBQSEpLgEGipAReTsLzYwWYxi8FgzGIbg+Vdli1pluf+cc6MRqORPbJ1dCSd7/v10stzzhzNPMfST8/8fs85v5+5OyIiIgBVcQcgIiL9h5KCiIgUKCmIiEiBkoKIiBQoKYiISIGSgoiIFCgpiIhIgZKCiIgUKCmIiEiBkoKIiBRUxx1AT40fP96nT58edxgiIgPKc889t8HdG3Z33IBLCtOnT2fp0qVxhyEiMqCY2buVHKfhIxERKVBSEBGRAiUFEREpUFIQEZGCyJKCmf3czNab2SvdPG9mdr2ZrTSzl8zsmKhiERGRykTZU7gVmLOL588EZoZf84CfRRiLiIhUILJLUt39UTObvotDzgF+5cF6oE+b2Wgzm+ju66KKSSQKmWyOba0Ztuxop6UtSzqXI5N1Mtkc6ZyTzeW6fE+5VXBL92mhXCl1yL4jmDp2WKTvEed9CpOB1UXba8J9XZKCmc0j6E2w33779UlwIt35YGsr97+yjsdXbuSND5tZvXlH2T/yIr3tuk8fxiUnTov0PQbEzWvufhNwE0BjY6Oan8SiuTXN9/77de5Yupp01pk+bhiHTx7FOUdNYmx9LaOH1TB8SA3VKaOmqir4N2WkqqqwMq9nZXZayZHljpHkmjiqLvL3iDMprAWmFm1PCfdF4ubHVvGDB97g+f/zSYbWpqJ6Gxmk3t+yk8/e/AzvbmzhsydM49KTp7N/w/C4wxLpdXEmhYXAfDNbAJwAbI2ynuAOO9NZMrkcoKQglWtpy3DJLc+wobmN2790IifsPy7ukEQiE1lSMLPbgdnAeDNbA/wdUAPg7v8KLALOAlYCO4BLo4oFoDoV9MOzOY0+Sc9cd99rvL2hhd988QQlBBn0orz66OLdPO/Al6N6/1LVVUFSyCgpSA+88WEzC5a8x6UfncFHDxgfdzgikUvMHc2pquBU1VOQnvjpwyupr63mitMOjDsUkT6RmKSgnoL01JYd7Sx65QPOO2YyY+pr4w5HpE8kJimk8kkh2/VGIpFy7ln2Pu2ZHBccN3X3B4sMEolJCvlCs3oKUqkHX1/PjPH1fGTSqLhDEekzyUkKqilID7Smszz11kZmH7zb1QtFBpXEJIWO4SMlBdm9Z9/eRFsmx+yDJ8QdikifSkxSyBea1VOQSjz/3maqDBqnjYk7FJE+lZikkCrUFFRolt17cfUWZk4YQf2QATE9mEivSUxS0CWpUil3Z9nqLRw5VQVmSZ7EJAXVFKRSazbvZPOONEdOHR13KCJ9LjFJoSalq4+kMm+ubwaCBU1EkiYxSaHQU1BNQXbjrfUtAOw/XlNjS/IkJino6iOp1FtN2xlXX6upLSSREpMUUio0S4XeatrOAVpARxIqMUkhf0ezCs2yO281tXDAhPq4wxCJRXKSgu5TkAo0t6bZ1NLOfmOVFCSZkpMUVFOQCqzb2grApNHRL5Au0h8lJimopiCVeH/LTgAmjx4acyQi8UhMUtAsqVKJ97fkewpKCpJMkSYFM5tjZivMbKWZXVXm+Wlm9qCZvWRmj5jZlKhiUU9BKrFu606qDCaMGBJ3KCKxiCwpmFkKuBE4E5gFXGxms0oO+2fgV+5+BHAt8A9RxVOtldekAmu37GTfkXVUpxLTiRbpJMrf/OOBle6+yt3bgQXAOSXHzAIeCh8/XOb5XpO/+kjDR7Ir72/ZqaEjSbQok8JkYHXR9ppwX7FlwLnh488AI8xsXBTBFO5TUFKQXVi/rY19RunKI0muuPvIfwOcamYvAKcCa4Fs6UFmNs/MlprZ0qampj16o5QuSZUKbNjeRsNw1RMkuaJMCmuBqUXbU8J9Be7+vruf6+5HA38b7ttS+kLufpO7N7p7Y0PDnq2ZW62ps2U32jM5trVmGKc5jyTBokwKS4CZZjbDzGqBi4CFxQeY2Xgzy8dwNfDzqIKpqjLMdEezdG9TSzsA49RTkASLLCm4ewaYDywGXgPucPdXzexaM5sbHjYbWGFmbwD7AP83qngg6C2opiDd2bC9DYBxw9VTkOSKdAFad18ELCrZd03R4zuBO6OMoVh1VZVqCtKtjWFPYbySgiRY3IXmPlVdZaopSLc2hj2FsfUaPpLkSlRSSKWMrGoK0o2N2/M1BfUUJLkSlRRUU5Bd2dDSRm2qihFDIh1VFenXEpYUqkhrmgvpxsbt7YwbXouZxR2KSGwSlRRqqlVTkO5t2dHO6GEaOpJkS1ZSqKqiXT0F6ca2nRlGDdXQkSRbspJCSsNH0r1trWlG1tXEHYZIrJKVFKqNtIaPpBvbdqYZNVRJQZItWUlBPQXZha0704xUUpCEU1IQIVh8qaU9q+EjSbxEJYXaVJWGj6Ss5tYMACNVaJaES1RSqEmZegpS1rbWNIBqCpJ4CUsKVbRnlBSkq207w56Cho8k4ZKVFKpVU5Dytu4MegoqNEvSJSopqKYg3ckPH6mmIEmXqKRQXaWagpS3Ld9T0PCRJFyikoKGj6Q7KjSLBBKVFGpVaJZubNuZocpgWG0q7lBEYpWopFCT0noKUl5Le4b6IdWaNlsSL2FJQcNHUt6Otiz1tSoyi0SaFMxsjpmtMLOVZnZVmef3M7OHzewFM3vJzM6KMp6a8Oojd/UWpLOW9gzDhmjoSCSypGBmKeBG4ExgFnCxmc0qOezbwB3ufjRwEfDTqOIBqK0OTleXpUqpHe1Z1RNEiLancDyw0t1XuXs7sAA4p+QYB0aGj0cB70cYDzWpYLxYQ0hSqqUtwzANH4lEmhQmA6uLtteE+4p9B7jEzNYAi4ArIoyHmlS+p6CkIJ3taM9Sr56CSOyF5ouBW919CnAWcJuZdYnJzOaZ2VIzW9rU1LTHb5ZPClqSU0oFNQX1FESiTAprgalF21PCfcUuA+4AcPengDpgfOkLuftN7t7o7o0NDQ17HFDH8JFqCtJZcPWRegoiUSaFJcBMM5thZrUEheSFJce8B/wJgJkdSpAU9rwrsBuF4SPdwCYldrSrpiACESYFd88A84HFwGsEVxm9ambXmtnc8LBvAF8ys2XA7cDnPcLrRVVTkHLcPagp6JJUESL9aOTuiwgKyMX7ril6vBw4OcoYiqmmIOW0Z3Nkcq6eggjxF5r7VG11UFPIqKYgRXa0ZQFUUxAhYUlBw0dSTkt7sOqarj4SSWhS0PCRFNvRnu8pKCmIJDIp6JJUKdbSlu8paPhIJFFJoVaXpEoZ6imIdEhUUqip1txH0lWhp6BCs0jCkoJqClJGvqcwVElBJGFJoUo1BemqNR0mhRolBZFEJYX8egpap1mK5ZNCnZKCSLKSQl1NcLr5PwIiAK3hh4T874dIkiWqFeQ/CbZmlBSkQ/5DwpBq9RREEpUUhoTDR63tSgrSoTWdoyZlpKos7lBEYpeopGBm1NVUFYYLRADaMlnq1EsQARKWFCC4wmSnegpSpDWdY4iKzCJAApNCXU1KhWbppC2dVZFZJJS4ljC0JsVOJQUp0prJ6nJUkVDiksKQmhStadUUpENrOle4CEEk6RLXEupqqmjTJalSpDWtnoJIXuKSggrNUqotk1NNQSSUuJZQV5PSzWvSSWtal6SK5EWaFMxsjpmtMLOVZnZVmed/ZGYvhl9vmNmWKOMB9RSkKw0fiXSIbFURM0sBNwKfBNYAS8xsobsvzx/j7l8vOv4K4Oio4skbUlOlQrN0EtynkLhOs0hZUbaE44GV7r7K3duBBcA5uzj+YuD2COMBdJ+CdNWWyWreI5FQlElhMrC6aHtNuK8LM5sGzAAeijAeQPcpSFetaRWaRfL6S0u4CLjT3cv+tTazeWa21MyWNjU17dUb1Q+pZkd7llxOC+1IoE03r4kURJkU1gJTi7anhPvKuYhdDB25+03u3ujujQ0NDXsV1PAhQeNvac/s1evI4JDNOems6+ojkVCUSWEJMNPMZphZLcEf/oWlB5nZIcAY4KkIYykYPqQGgJY2DSFJ8apr/aXTLBKvyFqCu2eA+cBi4DXgDnd/1cyuNbO5RYdeBCxw9z4Zz6kPewrb29J98XbSz2kpTpHOIrskFcDdFwGLSvZdU7L9nShjKDWiLjjl7eopCFqKU6RURS3BzL5qZiMtcIuZPW9mZ0QdXBTyw0fbW1VTEC3FKVKq0o9HX3D3bcAZBOP/fwF8L7KoItQxfKSkINCWVk9BpFilLSG/eO1ZwG3u/mrRvgFlRL6noKQgUJgxVz0FkUClSeE5M3uAICksNrMRwICcKyLfU2hRUhAgnQ2ub6hJqacgApUXmi8DjgJWufsOMxsLXBpdWNEZXig0KykItIeF5lotsiMCVN5TOAlY4e5bzOwS4NvA1ujCis6Q6hQ1KVNSEADS2SAp1KQG5GioSK+rNCn8DNhhZkcC3wDeAn4VWVQRGz6kWlcfCRAssAPqKYjkVdoSMuHNZecAN7j7jcCI6MKK1vC6appbdfOadPQUtEazSKDSmkKzmV1NcCnqKWZWBdREF1a0xg6rZfMOJQXpqCmo0CwSqLQlXAi0Edyv8AHB5HbfjyyqiI2pr2Xzjva4w5B+IN9T0PCRSKCilhAmgt8Ao8zsbKDV3QdsTWHssFo2tSgpCLRn1VMQKVbpNBcXAM8CfwZcADxjZudHGViUxtTXsllJQdAlqSKlKq0p/C1wnLuvBzCzBuCPwJ1RBRalsfW1tLRntWC7FHoKteopiACV1xSq8gkhtLEH39vvjBlWC8AWFZsTT4Vmkc4q7Sncb2aL6Vgd7UJKpsQeSMbWBxdObWppZ99RdTFHI3FKZ3OkqoxUlW5eE4EKk4K7f9PMzgNODnfd5O7/GV1Y0RpbPwSAjS1tMUcicWvP5DR0JFKk4kV23P0u4K4IY+kz+44MegcfbG2NORKJWzrrmuJCpMguk4KZNQPllsk0wN19ZCRRRWzCyKCnoKQgbZkctZo2W6Rgl0nB3QfsVBa7UleTYlx9Leu2KSkkXTqbo1Y9BZGCxA6m7juqTj0FCWoKukdBpCDS1mBmc8xshZmtNLOrujnmAjNbbmavmtlvo4yn2MRRdaxTUki8dDany1FFilRcaO4pM0sBNwKfBNYAS8xsobsvLzpmJnA1cLK7bzazCVHFU2rKmGE8vWoT7o6Zhg+SSj0Fkc6ibA3HAyvdfZW7twMLCKbeLvYl4EZ33wxQcoNcpGaMr2d7W4amZl2WmmTtWSUFkWJRtobJwOqi7TXhvmIHAQeZ2RNm9rSZzYkwnk72b6gHYNWGlr56S+mH2jMaPhIpFndrqAZmArOBi4F/N7PRpQeZ2TwzW2pmS5uamnrljWeMD5NCk5JCkqWzOS2wI1IkytawFphatD0l3FdsDbDQ3dPu/jbwBkGS6MTdb3L3RndvbGho6JXgJo0aypDqKt7esL1XXk8GpnYVmkU6ibI1LAFmmtkMM6sFLgIWlhxzN0EvATMbTzCctCrCmAqqqoyZ+wznlbXb+uLtpJ9KZ1zTXIgUiaw1uHsGmA8sBl4D7nD3V83sWjObGx62GNhoZsuBh4FvuvvGqGIqddz0sbywenNhpkxJnvZsjhoNH4kURHZJKoC7L6JkNlV3v6bosQNXhl997oQZY/nFE+/w8totHDttbBwhSMw0IZ5IZ4luDcdNDxLBM29vijkSiUtwSaruUxHJS3RSGDd8CDMnDOeZVUoKSRXMfZToZiDSSeJbwwn7j+W5dzeTyaqukES6T0Gks8S3huNnjGN7W4bX1jXHHYrEQNNciHSW+NZwwoygrvDUqg0xRyJ9LZdzMjlXT0GkSOJbwz4j6zh04kjuf+WDuEORPtYeDhmqpyDSQa0BOPuIiTz/3hbWbtkZdyjShzK5YFFBLccp0kFJAfjU4RMBWPTSupgjkb6Uv7igukrNQCRPrQGYPr6ewyaP5N6XlRSSRD0Fka6UFEJnHzGJZau3sHrTjrhDkT6SyQZJIaWegkiBWkMoP4R0n3oLiZHODx+ppyBSoKQQmjp2GEdOHc19qiskRjYcPqquUlIQyVNSKHL24RN5ee1W3tFqbImQyeV7CmoGInlqDUXOOkJDSElSKDSrpyBSoKRQZPLooRy932jdyJYQHYVmJQWRPCWFEqcfug8vr93K+m2tcYciEeu4JFXNQCRPraHEaYdMAOCRFU0xRyJRy9+8pp6CSAclhRKH7DuCSaPqePD1D+MORSKWDoePdEmqSAclhRJmxmmHTuCxNzfQlsnGHY5EqOOSVDUDkTy1hjJOO2QCO9qzPKtlOge1dE43r4mUijQpmNkcM1thZivN7Koyz3/ezJrM7MXw64tRxlOpk/YfT22qisfe1BoLg1k2m78kVZ+NRPIiaw1mlgJuBM4EZgEXm9msMof+zt2PCr9ujiqenhham+LYaWN49A0Vmwez/M1rKjSLdIjyI9LxwEp3X+Xu7cAC4JwI369XnXbIBF7/oJk1mzVB3mCVLzRrllSRDlEmhcnA6qLtNeG+UueZ2UtmdqeZTS33QmY2z8yWmtnSpqa++fR+6sENADyuIaRBK19oVk9BpEPcg6n3ANPd/QjgD8Avyx3k7je5e6O7NzY0NPRJYDMnDGffkXWqKwxi+VlSdfOaSIcoW8NaoPiT/5RwX4G7b3T3tnDzZuDYCOPpETPjlJnjeXzlhsInShlcCpekavhIpCDKpLAEmGlmM8ysFrgIWFh8gJlNLNqcC7wWYTw9dspBDWzdmebltVvjDkUikNbwkUgXkSUFd88A84HFBH/s73D3V83sWjObGx72FTN71cyWAV8BPh9VPHviYweOxwweWbE+7lAkAtn88JEuSRUpqI7yxd19EbCoZN81RY+vBq6OMoa9Mba+lsZpY7j/lQ/42ukHxR2O9LL8hHgpDR+JFOgj0m6cedhEXv+gmVVN2+MORXpZWjeviXSh1rAbcw7bF4D/1hoLg05WN6+JdKGksBuTwoV37tXazYOObl4T6UpJoQJ/esQkXlu3jTc/bI47FOlF2ZyTqjLMlBRE8pQUKnD2kROpMrhHvYVBJZ3LaehIpISSQgUmjKijcdpYHnhVdYXBJJN1apQURDpRUqjQmYfvy+sfNLP8/W1xhyK9JD98JCIdlBQqdO7RU6irqeI3z7wbdyjSS9LZnOY9EimhFlGhUcNqOOuwiSxc9r6W6RwksjnXvEciJZQUemDuUZNobs3wyAotvjMYpLOu9ZlFSqhF9MDJB45nXH0tdz23Ju5QpBdkcjn1FERKKCn0QE2qivMbp/Dg6+tZt3Vn3OHIXsqo0CzShZJCD11ywjRSZvz4j2/GHYrspUw2p3mPREqoRfTQ1LHDOO/YKdz94lq2tabjDkf2ggrNIl0pKeyBS07cj9Z0jl8+8U7cocheCArNSgoixZQU9sBHJo3i9EMncPPjb9Os3sKAFfQU1AREiqlF7KErTpvJ1p1pbn/2vbhDkT2UzmruI5FSSgp76Mipo/noAeO45fG3ac/k4g5H9kAm55o2W6SEksJeuPzUA/hwWxt3v7g27lBkDwSXpKoJiBSLtEWY2RwzW2FmK83sql0cd56ZuZk1RhlPbztl5ng+MmkkP/7jm+otDEDBJanqKYgUiywpmFkKuBE4E5gFXGxms8ocNwL4KvBMVLFExcz45v84mLVbdvLrpzVR3kCjS1JFuoqyp3A8sNLdV7l7O7AAOKfMcd8F/hFojTCWyJx6UAOnzBzPj/7wBlt2tMcdjvRAOpvT3EciJaJsEZOB1UXba8J9BWZ2DDDV3e+LMI5ImRnfmnMIzW0Z/kV3OQ8oGfUURLqI7WOSmVUBPwS+UcGx88xsqZktbWrqfzOUHjZ5FBc2TuW3z7zHOxta4g5HKpTJau4jkVJRJoW1wNSi7SnhvrwRwGHAI2b2DnAisLBcsdndb3L3RndvbGhoiDDkPXflGQdRnTJueHhl3KFIhTI5zX0kUirKFrEEmGlmM8ysFrgIWJh/0t23uvt4d5/u7tOBp4G57r40wpgis8/IOs45ajILl72vGVQHCBWaRbqKLCm4ewaYDywGXgPucPdXzexaM5sb1fvG6a9nH4C7c/2D6i0MBJr7SKSr6ihf3N0XAYtK9l3TzbGzo4ylL0wdO4w/P34/fv3Me8z7+P7MGF8fd0iyC5lsTnMfiZRQi+hl80+bSW2qih/+4Y24Q5HdyOTUUxAppaTQyxpGDOELH5vOPcve5+U1W+MOR3ZBl6SKdKWkEIG/PPUARg+r4bv3LSeX87jDkTLcnazmPhLpQi0iAiPravjGGQfz7NubeOKtDXGHI2VkwmStuY9EOlNSiMgFjVMYW1/LX9zyLJmsJsvrb7JhUlChWaQztYiIDKlOMf8TBwLw1QUvxhyNlEqHiVqFZpHOlBQi9IWPzWDiqDrue3kdqzftiDscKZLJ5nsKSgoixZQUIvb7y08C4PuLV8QciRTL1xTUUxDpTEkhYlPGDGP+Jw5k4bL3ueXxt+MOR0KZXDh8pJqCSCdqEX3ga6fPpL42xXfvXa55kfqJwvCRegoinSgp9IHqVBW/v/yjAJzxo0djjkagaPhINQWRTpQU+sisSSM5cf+xNLdmuGfZ+3GHk1i5nLO9LcOqpu0AWnlNpESkE+JJZ7dddgJnX/84V97xIsdOG8Ok0UPjDqlfcHceen0944YPYfq4YYweVtvp+a070yxbvYXr7lvOTy4+hgMa6nnszQ2Mqa9l9aYdXHH7C4VjT5gxlmfe3lTYPmPWPlx91qHUpIxnVm3iG79f1um1hw9RExApZu4DaxqGxsZGX7p0QC65AMDK9c2c/sNHGVdfy/1f+zgNI4bEHVKv2dTSzitrt/Lxgxpwd664/QU2tbTz9oYW1m0NluC+9dLjmH3wBADe/LCZc3/2JM2tmS6vde7Rk5l71CTm3fYc7ZnevflvSHUVbZkc4+prefibsxlZV9Orry/SH5nZc+7eZRGzLscpKfS9Hzywgp88FKy58Ocn7Me1cz/SL66CcXfMejbGns7muP+VD3iraXvFa1R/4uAGHl7Rs2VVzWC/scN4d2PH/R6jh9WQyTr/dP4RfGTSSJ57dzNX3rGMb3/qUL54yv5ksjlue/pd3t24g1uffIfJo4eycP7JjBs+eBKxSKWUFPq5p97ayFcWvEBTcxsAv/niCZx84PjI3m9TSzu/fPId3tnYwn+92FHTuLBxKtf86Swu/cUSnn1nU6fvOfuIiXxrziFMGj2UhcvWsqqphfXb2vjsifsx94Ynyr5PdZUVirgAC+adyJFTRjO0NsWvnnqHa/7r1U7H/8O5h/PRA8YxbVyw9sTG7W2s2byTFR808/Sqjew7qo6/OeNgqnSVkMheUVIYIH7xxNv8/T3LC9u3XXY8x88Yy5Dq1F69rrvzVlML+4+v5/JfP8cDyz/c21C7VV1lzD1yEuccPZlTD2qgNZ1lSHVV2V7HzY+t4r1NO/ja6QcxZlhNj3smIrJnlBQGkOff28y5P32ysG0G1879CLMPnsDUscN2+b2t6SzvbtzBwfuO4N2NLSxYspqn3trIi6u3dDl2ypihnLj/OL78iQOZPm4YZsaP//gmP/rjG1RXGSuuO5N0NkddTYqWtgx/WP4hV97xIvkP/pNHD+X4GWN5ZMV6/tecQ/iTQycwYURdr/5fiEg0lBQGoObWNN9ZuJy7nl/T5bn/edI0fvXUuwD8bt6JPPT6epav28Zjb1Y2Nffr351DXc3e9T5EZOBSUhjAbnjoTZ5atZGm5jbe+HB7j763uso495jJ/L/PHN4vitci0j9UmhQivUjbzOYAPwZSwM3u/r2S5y8Hvgxkge3APHdf3uWFEmb+aTOZf9rMwvaCZ9/jhodX8uOLjqaupopPXf84x04bw5mH7cslJ06jribFjvYMw2p1zb2I7J3IegpmlgLeAD4JrAGWABcX/9E3s5Huvi18PBf4a3efs6vXTUJPQUSkt1XaU4hyfOF4YKW7r3L3dmABcE7xAfmEEKoHBtZYlojIIBPleMNkYHXR9hrghNKDzOzLwJVALXBahPGIiMhuxF6JdPcb3f0A4FvAt8sdY2bzzGypmS1taurZnbAiIlK5KJPCWmBq0faUcF93FgCfLveEu9/k7o3u3tjQ0NCLIYqISLEok8ISYKaZzTCzWuAiYGHxAWY2s2jzU0Blk+eIiEgkIqspuHvGzOYDiwkuSf25u79qZtcCS919ITDfzE4H0sBm4HNRxSMiIrsX6YXt7r4IWFSy75qix1+N8v1FRKRnYi80i4hI/zHgprkwsybg3bjj2EvjgcomLer/dC79z2A5D9C59KZp7r7bK3UGXFIYDMxsaSV3Fg4EOpf+Z7CcB+hc4qDhIxERKVBSEBGRAiWFeNwUdwC9SOfS/wyW8wCdS59TTUFERArUUxARkQIlBRERKVBSEBGRAiWFfsbMPm1m/25mvzOzM+KOp6fMbH8zu8XM7ow7lr1lZvXhlO1nxx3L3jCzU8zsX83sZjN7Mu54eqrc71T4s/ll2FY+G2d8PdHNuRwa/nzuNLO/ijM+UFLoVWb2czNbb2avlOyfY2YrzGylmV21q9dw97vd/UvA5cCFUcZbqZ6cV7jS3mXxRLpre/Dz+RZwR99GWZke/kwec/fLgXuBX8YRb6le+J06F7gzbCtz+yjssvb2XNz9tfDncwFwct9FXp6SQu+6Fei0xnS4VvWNwJnALOBiM5tlZoeb2b0lXxOKvvXb4ff1B7dS4Xn1fWg9ciuV/3w+CSwH1vd1kBW6lZ7/TP4c+G1fBbgbt7J3v1NT6FjZMRtRjJW6lb1sH+Ea9fdRMoFoHJQUepG7PwpsKtlddq1qd3/Z3c8u+VpvgX8E/tvdn+/rcyinJ+fV58H1QA/PYzZwIsEf0i+ZWb9qKz39mZjZfsBWd2/u20jL64XfqTUEiQFi/jvWG+2BhikhAAAGIElEQVTD3Re6+5lA7ENh/eoXfZAqt1b15F0cfwVwOnC+mV0eZWB7qex5mdk4M/tX4Ggzuzqe0Hqk7Hm4+9+6+9cIPln/u7vnYomuZ3b1u3YZ8Is+j6hnevI79R/AeWb2M+CePo6zEhWfi5nNNrPrzezf6Ac9hUjXU5Cec/frgevjjmNPuftGgnrIoODut8YdQ29w97+LO4Y9Ve53yt1bgEvjiWjPdXMujwCPxBFPOeopRK+na1UPFIPlvAbLecDAP5eBHn+xAXsuSgrR2+1a1QPUYDmvwXIeMPDPZaDHX2zAnouSQi8ys9uBp4CDzWyNmV3m7hkgv1b1a8Ad7v5qnHH21GA5r8FyHjDwz2Wgx19sMJ0LaEI8EREpop6CiIgUKCmIiEiBkoKIiBQoKYiISIGSgoiIFCgpiIhIgZKCRM7MtvfBe8zd3bTkEbznbDP76B5839Fmdkv4+PNmdkPvR9dzZja9dPrnMsc0mNn9fRWT9D0lBRkwwumIywpnmfxeBO+5q/nBZgM9TgrA/2aAzm/l7k3AOjOLfd5/iYaSgvQpM/ummS0xs5fM7O+L9t9tZs+Z2atmNq9o/3Yz+4GZLQNOMrN3zOzvzex5M3vZzA4Jjyt84jazW8NZJ580s1Vmdn64v8rMfmpmr5vZH8xsUf65khgfMbN/MbOlwFfN7E/N7Bkze8HM/mhm+5jZdIKJzb5uZi9asLpZg5ndFZ7fknJ/OM1sBHCEuy8r89x0M3so/L95MJzuGjM7wMyeDs/3unI9LwtWIrvPzJaZ2StmdmG4/7jw/2GZmT1rZiPC93ks/D98vlxvx8xSZvb9op/VXxY9fTf9YIpniYi760tfkX4B28N/zwBuAozgA8m9wMfD58aG/w4FXgHGhdsOXFD0Wu8AV4SP/xq4OXz8eeCG8PGtwO/D95hFMK89wPkEUxNXAfsCm4Hzy8T7CPDTou0xdNz9/0XgB+Hj7wB/U3Tcb4GPhY/3A14r89qfAO4q2i6O+x7gc+HjLwB3h4/vBS4OH1+e//8sed3zCKb4zm+PAmqBVcBx4b6RBDMjDwPqwn0zgaXh4+nAK+HjecC3w8dDgKXAjHB7MvBy3L9X+ormS1NnS186I/x6IdweTvBH6VHgK2b2mXD/1HD/RoJVte4qeZ3/CP99jmBZxnLu9mANhOVmtk+472PA78P9H5jZw7uI9XdFj6cAvzOziQR/aN/u5ntOB2aZWX57pJkNd/fiT/YTgaZuvv+kovO5Dfinov2fDh//FvjnMt/7MvADCxZoutfdHzOzw4F17r4EwN23QdCrAG4ws6MI/n8PKvN6ZwBHFPWkRhH8TN4mWI1uUjfnIAOckoL0JQP+wd3/rdNOs9kEf1BPcvcdZvYIUBc+3erupcsttoX/Zun+d7it6LF1c8yutBQ9/gnwQ3dfGMb6nW6+pwo40d1bd/G6O+k4t17j7m+Y2THAWcB1ZvYg8J/dHP514EPgSIKYy8VrBD2yxWWeqyM4DxmEVFOQvrQY+IKZDQcws8kWrEs9CtgcJoRDCJbBjMITBKt1VYW9h9kVft8oOubC/1zR/mZgRNH2AwQr5wEQfhIv9RpwYDfv8yTBFMsQjNk/Fj5+mmB4iKLnOzGzScAOd/818H3gGGAFMNHMjguPGREWzkcR9CBywF8A5Qr4i4G/MrOa8HsPCnsYEPQsdnmVkgxcSgrSZ9z9AYLhj6fM7GXgToI/qvcD1Wb2GvA9gj+CUbiLYFnE5cCvgeeBrRV833eA35vZc8CGov33AJ/JF5qBrwCNYWF2OWVWoHP314FRYcG51BXApWb2EsEf66+G+78GXBnuP7CbmA8HnjWzF4G/A67zYG3gC4GfhIX6PxB8yv8p8Llw3yF07hXl3Uzw//R8eJnqv9HRK/sEwSLzMghp6mxJlPwYv5mNA54FTnb3D/o4hq8Dze5+c4XHDwN2urub2UUEReduF4GPmpk9Cpzj7pvjikGio5qCJM29ZjaaoGD83b5OCKGfAX/Wg+OPJSgMG7CF4MqkWJhZA0F9RQlhkFJPQUREClRTEBGRAiUFEREpUFIQEZECJQURESlQUhARkQIlBRERKfj/wNMHgkWH6Q4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot(n_skip=0, n_skip_end=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a143dbf7f0ff47c1abbbd4dc95cf999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                  \n",
      "    0      0.216088   0.1515    \n",
      "    1      0.206034   0.135275                                  \n",
      "    2      0.205923   0.131817                                  \n",
      "    3      0.200415   0.128312                                  \n",
      "    4      0.198345   0.126672                                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1266719105116333]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=5, wds=wd, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f75f2a129c4f9f8d6c21499d6cb6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                  \n",
      "    0      0.210491   0.135097  \n",
      "    1      0.204235   0.130115                                  \n",
      "    2      0.205884   0.130088                                  \n",
      "    3      0.196507   0.124716                                  \n",
      "    4      0.199349   0.122149                                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1221494674602016]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=5, wds=wd, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf2488f31094b10aa465fbb97680cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                  \n",
      "    0      0.156351   0.117418  \n",
      "    1      0.161498   0.118112                                  \n",
      "    2      0.160972   0.117604                                   \n",
      "    3      0.157842   0.117846                                  \n",
      "    4      0.160269   0.117712                                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.117712354837357]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 1, cycle_len=5, wds=wd, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'devise_trained_full_imagenet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [devise]",
   "language": "python",
   "name": "devise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
